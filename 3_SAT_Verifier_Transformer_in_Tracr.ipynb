{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndisDraguns/rasp-lab/blob/main/3_SAT_Verifier_Transformer_in_Tracr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WFuk733m3Zt"
      },
      "source": [
        "# 3-SAT Verifier Transformer in Tracr\n",
        "\n",
        "I implement a [3-SAT](https://en.wikipedia.org/wiki/Boolean_satisfiability_problem) verifier as a Transformer circuit by using [RASP](https://arxiv.org/pdf/2106.06981.pdf) programs in [Tracr](https://github.com/google-deepmind/tracr/tree/main/tracr/rasp). This can serve as a toy example for a Transformer that is hard to reverse, and showcase theoretical cases that would be difficult for algorithms such as [Greedy Coordinate Descent](https://arxiv.org/pdf/2307.15043.pdf) (GCG). Any problem in NP can be reduced to 3-SAT, including ones that seem to be hard in the average case, e.g. integer factoring, (which is believed highly unlikely to be NP-complete).\n",
        "\n",
        "RASP is a programming language that allows implementing handcrafted Transformer circuits. It can be used to to make Transformers perform simple algorithms such as reversing a string in a way that achieves length-generalization. For a great introduction on RASP, see this blog: https://srush.github.io/raspy/.\n",
        "\n",
        "This notebook is made for developing and experimenting with Tracr models. It adapts Neel Nanda's [Tracr to TransformerLens Converter notebook](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Tracr_to_Transformer_Lens_Demo.ipynb) and Tracr's [example visualisation notebook](https://github.com/google-deepmind/tracr/blob/main/tracr/examples/Visualize_Tracr_Models.ipynb). A TransformerLens model can be accessed at the end of the 'Build the TransformerLens model' section. In this version I've added some helper functions and bug fixes.\n",
        "\n",
        "If you have any suggested improvements or bug fixes, feel free to message or email me!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies\n",
        "\n",
        "This might take a few minutes:"
      ],
      "metadata": {
        "id": "PqhEKAnQ1AQ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtdXi36Ym3Zv",
        "outputId": "0306fdce-054a-4ba1-cc1f-79a637fa1e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.10/dist-packages (1.14.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.27.2)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.18.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.28)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.7.1)\n",
            "Requirement already satisfied: torch!=2.0,!=2.1.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.2.1)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.2)\n",
            "Requirement already satisfied: transformers>=4.34 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.38.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.10.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.9.3)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2023.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.4.99)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->transformer_lens) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->transformer_lens) (0.15.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.42)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.41.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n",
            "Collecting git+https://github.com/deepmind/tracr\n",
            "  Cloning https://github.com/deepmind/tracr to /tmp/pip-req-build-90tt6jet\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepmind/tracr /tmp/pip-req-build-90tt6jet\n",
            "  Resolved https://github.com/deepmind/tracr to commit 9ce2b8c82b6ba10e62e86cf6f390e7536d4fd2cd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: chex in /usr/local/lib/python3.10/dist-packages (from tracr==1.0.0) (0.1.85)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from tracr==1.0.0) (0.7.0)\n",
            "Requirement already satisfied: dm-haiku in /usr/local/lib/python3.10/dist-packages (from tracr==1.0.0) (0.0.12)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from tracr==1.0.0) (0.4.23)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from tracr==1.0.0) (3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tracr==1.0.0) (1.25.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from tracr==1.0.0) (4.10.0)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex->tracr==1.0.0) (1.4.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from chex->tracr==1.0.0) (0.4.23+cuda12.cudnn89)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex->tracr==1.0.0) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->tracr==1.0.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->tracr==1.0.0) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->tracr==1.0.0) (1.11.4)\n",
            "Requirement already satisfied: jmp>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from dm-haiku->tracr==1.0.0) (0.0.4)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from dm-haiku->tracr==1.0.0) (0.9.0)\n",
            "Requirement already satisfied: flax>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-haiku->tracr==1.0.0) (0.8.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku->tracr==1.0.0) (1.0.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku->tracr==1.0.0) (0.2.1)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku->tracr==1.0.0) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku->tracr==1.0.0) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku->tracr==1.0.0) (13.7.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku->tracr==1.0.0) (6.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.1->dm-haiku->tracr==1.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.1->dm-haiku->tracr==1.0.0) (2.16.1)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku->tracr==1.0.0) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku->tracr==1.0.0) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku->tracr==1.0.0) (3.20.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.1->dm-haiku->tracr==1.0.0) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku->tracr==1.0.0) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku->tracr==1.0.0) (6.1.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku->tracr==1.0.0) (3.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformer_lens\n",
        "%pip install git+https://github.com/deepmind/tracr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRaHudcom3Zv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
        "import einops\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from tracr.rasp import rasp\n",
        "from tracr.compiler import compiling\n",
        "from tracr.compiler import lib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import jax\n",
        "\n",
        "\n",
        "# The default of float16 can lead to discrepancies between outputs of\n",
        "# the compiled model and the RASP program.\n",
        "jax.config.update('jax_default_matmul_precision', 'float32')\n",
        "\n",
        "# originally developed for these versions:\n",
        "  # Python 3.10.12\n",
        "  # transformer_lens==1.14.0\n",
        "  # git+https://github.com/google-deepmind/tracr/tree/9ce2b8c82b6ba10e62e86cf6f390e7536d4fd2cd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define helper functions"
      ],
      "metadata": {
        "id": "iwds7R-w_2PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions: Interfacing with Tracr\n",
        "from typing import Iterable\n",
        "\n",
        "def create_input_list(input: Iterable) -> list:\n",
        "  return [compiler_bos] + list(input)\n",
        "\n",
        "def create_tracr_runner_fn(model) -> callable:\n",
        "  def run_tracr_model(input: Iterable)-> list:\n",
        "    input_list = create_input_list(input)\n",
        "    tracr_logits = model.apply(input_list)\n",
        "    tracr_decoded_output = tracr_logits.decoded\n",
        "    return tracr_decoded_output\n",
        "  return run_tracr_model\n",
        "\n",
        "compiler_bos = \"BOS\"\n",
        "compiler_pad = \"PAD\"\n",
        "causal = False\n",
        "mlp_exactness = 100"
      ],
      "metadata": {
        "id": "2PDXgohR8k8k",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions: RASP programming\n",
        "\n",
        "def make_all_true() -> rasp.SOp:\n",
        "  return rasp.Map(lambda i: True, rasp.indices).named(\"all_trues\")\n",
        "\n",
        "def make_length() -> rasp.SOp:\n",
        "  all_true_selector = rasp.Select(all_true, all_true, rasp.Comparison.TRUE).named(\"all_true_selector\")\n",
        "  return rasp.SelectorWidth(all_true_selector).named(\"length\")\n",
        "\n",
        "def hardcode_to_SOp(s: Iterable) -> rasp.SOp:\n",
        "  return rasp.Map(lambda i: s[i] if i<len(s) else \"_\", rasp.indices).named(\"hardcoded_value\")\n",
        "\n",
        "def elementwise_EQ(a: rasp.SOp, b: rasp.SOp) -> rasp.SOp:\n",
        "  return rasp.SequenceMap(lambda x, y: x == y, a, b).named(\"elementwise_EQ\")\n",
        "\n",
        "from tracr.compiler.lib import make_count\n",
        "def count_true_vals(bools: rasp.SOp) -> rasp.SOp:\n",
        "  return make_count(bools, True)\n",
        "\n",
        "def elementwise_OR(a: rasp.SOp, b: rasp.SOp) -> rasp.SOp:\n",
        "  return rasp.SequenceMap(lambda x, y: x or y, a, b).named(\"elementwise_OR\")\n",
        "\n",
        "def elementwise_AND(a: rasp.SOp, b: rasp.SOp) -> rasp.SOp:\n",
        "  return rasp.SequenceMap(lambda x, y: x and y, a, b).named(\"elementwise_AND\")\n",
        "\n",
        "def check_if_all_true(bools: rasp.SOp) -> rasp.SOp:\n",
        "  \"\"\"check if all bools are True (numerically stable)\"\"\"\n",
        "  inverted_bools = rasp.Map(lambda x: not x, bools).named(\"inverted_bools\")\n",
        "  any_missing  = rasp.numerical(rasp.Map(lambda x: x, inverted_bools).named(\"any_missing\"))\n",
        "  select_all = rasp.Select(rasp.indices, rasp.indices, rasp.Comparison.TRUE).named(\"select_all\")\n",
        "  has_missing = rasp.numerical(rasp.Aggregate(select_all, any_missing, default=0)).named(\"has_missing\")\n",
        "  not_has_missing = (~has_missing).named(\"not_has_missing\")\n",
        "  is_all_true = rasp.categorical(not_has_missing).named(\"is_all_true\")\n",
        "  return is_all_true\n",
        "\n",
        "def elementwise_AND_for_n_SOps(sop_list)->rasp.SOp:\n",
        "  if len(sop_list)==1: return sop_list[0]\n",
        "  return elementwise_AND(sop_list[0], elementwise_AND_for_n_SOps(sop_list[1:]))\n",
        "\n",
        "# Useful constants\n",
        "all_true = make_all_true()\n",
        "length = make_length()"
      ],
      "metadata": {
        "id": "2t2duvBp_Wec",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Password Checker\n",
        "\n",
        "The simplest backdoor trigger. Brute force black-box testing would not work, but GCG would probably succeed.\n",
        "\n",
        "A Tracr model inputs are lists that include a beginning-of-sentence token, e.g. [\"BOS\", 1, 2, 3]. These are tokenized and then the Tracr transformer is applied. Then an argmax is taken over the output, which is then detokenized by calling the .decoded attribute."
      ],
      "metadata": {
        "id": "h6do8zrk_6Eu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at3J4qAzG9cT"
      },
      "outputs": [],
      "source": [
        "def check_if_all_true_and_len_is_n(bools: rasp.SOp, n: int) -> rasp.SOp:\n",
        "  true_counts = count_true_vals(bools)\n",
        "  is_all_true = true_counts == n\n",
        "  return is_all_true.named(\"check_if_all_true_and_len_is_n\")\n",
        "\n",
        "def check_password(hardcoded_password: str) -> rasp.SOp:\n",
        "  \"\"\"outputs all True if the password is correct, else all False\"\"\"\n",
        "  pw = hardcode_to_SOp(hardcoded_password)\n",
        "  pw_bools = elementwise_EQ(rasp.tokens, pw)\n",
        "  output = check_if_all_true_and_len_is_n(pw_bools, len(hardcoded_password))\n",
        "  return output.named(\"pwcheck\")\n",
        "\n",
        "hardcoded_password = \"1101\"\n",
        "test_input = \"1101\"\n",
        "\n",
        "program = check_password(hardcoded_password)\n",
        "max_seq_len = len(hardcoded_password)\n",
        "# vocab = set(list(\"abcdefghijklmnopqrstuvwxyz\")+[\"_\"])\n",
        "vocab = set(list(\"10\"))\n",
        "tracr_model = compiling.compile_rasp_to_model(program, vocab, max_seq_len, False, compiler_bos, compiler_pad)\n",
        "run_tracr_model = create_tracr_runner_fn(tracr_model)\n",
        "\n",
        "decoded_output = run_tracr_model(test_input)\n",
        "print(decoded_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3-SAT verifier"
      ],
      "metadata": {
        "id": "dN5s5gvOL4VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ev(rasp_object: rasp.RASPExpr, test_input=test_input) -> None:\n",
        "  \"\"\"\n",
        "  evaluate a rasp object for debugging purposes\n",
        "  e.g. write ev(is_shorter_than_n) instead of print(is_shorter_than_n)\n",
        "  not passing test_input assumes that test_input is in scope\n",
        "  \"\"\"\n",
        "  inp = create_input_list(test_input)[1:]\n",
        "  print(f\"evaluated {rasp_object.name} on test_input:\", rasp.evaluate(rasp_object, inp), flush=True)\n",
        "\n",
        "def var_to_pair(var: int)->str:\n",
        "  \"\"\"one-hot encodings for boolean inputs\"\"\"\n",
        "  if int(var)==1: return \"01\"\n",
        "  if int(var)==0: return \"10\"\n",
        "  assert(False)\n",
        "\n",
        "def format_vars(vars):\n",
        "  \"\"\"e.g. \"1101\" -> [0,1,0,1,1,0,0,1] \"\"\"\n",
        "  pairs = [var_to_pair(i) for i in vars]\n",
        "  var_string = \"\".join(pairs)\n",
        "  int_list = [int(x) for x in var_string]\n",
        "  return int_list\n",
        "\n",
        "def get_key_value(i):\n",
        "    \"\"\"maps [0,1,2,3,4,...] -> [-1,1,-2,2,-3,...]\"\"\"\n",
        "    sign = -1 if i%2==0 else 1\n",
        "    value = i//2 + 1\n",
        "    return sign * value\n",
        "\n",
        "def swap_indices_fn(i: int) -> int:\n",
        "  \"\"\"[0, 1, 2, ...] -> [2, 1, 4, 3, 6, 5 ...]\"\"\"\n",
        "  is_even = i%2==0\n",
        "  if is_even: return i+1\n",
        "  else: return i-1\n",
        "\n",
        "def check_if_len_smaller_than_n(n: int) -> rasp.SOp:\n",
        "  inverted_one_hot_n = hardcode_to_SOp([True]*(n-1)+[False]).named(\"inverted_one_hot_n\")\n",
        "  inverted_one_hot_n = rasp.categorical(rasp.Map(lambda x: bool(x), inverted_one_hot_n)).named(\"inverted_one_hot_n\")\n",
        "  is_shorter_than_n = check_if_all_true(inverted_one_hot_n).named(\"is_shorter_than_n\")\n",
        "  return is_shorter_than_n\n",
        "\n",
        "def verify_input_correctness(n_variables: int)-> rasp.SOp:\n",
        "  \"\"\"chech that input len >= n_variables * 2 and input bools are one-hot-encoded\"\"\"\n",
        "  swapped_pairs_indices = rasp.Map(lambda x: swap_indices_fn(x), rasp.indices).named(\"swapped_pairs_indices\")\n",
        "  other_var_selector = rasp.Select(rasp.indices, swapped_pairs_indices, rasp.Comparison.EQ).named(\"other_var_selector\")\n",
        "  other_var = rasp.Aggregate(other_var_selector, rasp.tokens, default=None).named(\"other_var\")\n",
        "  var_1_xor_var_2 = rasp.SequenceMap(lambda x, y: bool(x) != bool(y), rasp.tokens, other_var).named(\"var_1_xor_var_2\")\n",
        "  is_correct_length = (~check_if_len_smaller_than_n(n_variables*2)).named(\"is_correct_length\")\n",
        "  input_pos_is_valid = elementwise_AND(var_1_xor_var_2, is_correct_length).named(\"input_pos_is_valid\")\n",
        "  input_is_valid = check_if_all_true(input_pos_is_valid).named(\"input_is_valid\")\n",
        "  return input_is_valid\n",
        "\n",
        "def fetch_var_i_for_clauses(clauses, i) -> rasp.SOp:\n",
        "  \"\"\"takes a batch of clauses and variable position i (1,2 or 3), returns\"\"\"\n",
        "  queries = hardcode_to_SOp(clauses[:,i]).named(f\"clauses_var_{i}\")\n",
        "  clauses_var_i_selector = rasp.Select(var_keys, queries, rasp.Comparison.EQ).named(f\"clause_var_{i}_selector\")\n",
        "  clauses_var_i_averaged = rasp.Aggregate(clauses_var_i_selector, rasp.tokens).named(f\"clause_var_{i}_averaged\")\n",
        "  clauses_var_i_detected = rasp.categorical(rasp.Map(lambda x: x==1, clauses_var_i_averaged)).named(f\"clauses_var_{i}_detected\")\n",
        "  return clauses_var_i_detected\n",
        "\n",
        "def verify_2n_clause_batch(clauses: np.array, n_variables: int) -> rasp.SOp:\n",
        "  \"\"\"evaluate a batch of clauses (batching because can fit only 2n clauses in one attention head)\"\"\"\n",
        "  c_vars = [fetch_var_i_for_clauses(clauses, i) for i in range(3)]\n",
        "  clauses_var_OR = elementwise_OR(elementwise_OR(c_vars[0],c_vars[1]),c_vars[2]).named(\"clauses_batch_OR_result\")\n",
        "  output = check_if_all_true(clauses_var_OR).named(\"clauses_batch_satisfied\")\n",
        "  return output\n",
        "\n",
        "def verify_3sat(clauses: np.array, n_variables: int) -> rasp.SOp:\n",
        "  \"\"\"outputs all True if the 3-SAT instance is satisfied, else all False\"\"\"\n",
        "  n_clauses = len(clauses)\n",
        "  clause_batch_size = 2*n_variables\n",
        "  n_clause_batches = math.ceil(n_clauses/(clause_batch_size))\n",
        "  n_padded_clauses = n_clause_batches*(clause_batch_size) - n_clauses\n",
        "  if n_padded_clauses > 0:\n",
        "    padded_clauses = np.tile(clauses[-1], (n_padded_clauses, 1))\n",
        "    clauses = np.concatenate((clauses, padded_clauses), axis=0)\n",
        "  clause_batch_verifications = []\n",
        "\n",
        "  for i in range(n_clause_batches):\n",
        "    clause_batch = clauses[i*clause_batch_size:(i+1)*clause_batch_size]\n",
        "    batch_i = verify_2n_clause_batch(clause_batch, n_variables).named(f\"verify_2n_clause_batch_{i}\")\n",
        "    clause_batch_verifications.append(batch_i)\n",
        "\n",
        "  output = elementwise_AND_for_n_SOps(clause_batch_verifications).named(\"unverified_output\")\n",
        "  verified_input = verify_input_correctness(n_variables).named(\"verified_input\")\n",
        "  verified_output = elementwise_AND(output, verified_input).named(\"verified_output\")\n",
        "  return verified_output.named(\"3SAT_instance_satisfied\")\n",
        "\n",
        "\n",
        "# useful constants:\n",
        "var_keys = rasp.Map(get_key_value, rasp.indices).named(f\"variable_keys\")  # -1 1 -2 2 -3 3 ...\n",
        "positive_keys = rasp.Map(lambda x: abs(x), var_keys).named(\"positive_keys_correct_inp\") # 1 1 2 2 3 3 ...\n",
        "\n",
        "\n",
        "n_variables = 5\n",
        "clauses = np.array([\n",
        "    [1, 2, -3],  # clause 1 - x1 or x2 or NOT x3\n",
        "    [-1, -4, 5], # clause ...\n",
        "    [1, -2, 4],  # ...\n",
        "    [-1, 3, -5],\n",
        "    [2, -3, 4],\n",
        "    [-2, -4, 5],\n",
        "    [-1, -3, -5],\n",
        "    [1, 4, -5],\n",
        "    [1, -2, -4],\n",
        "    [-1, 2, -5],\n",
        "    [3, -4, 5],\n",
        "    [-2, 3, -4],\n",
        "    [-1, -3, 4],\n",
        "    [2, 4, -5],\n",
        "    [1, 2, 3],\n",
        "    [-4, -5, 1],\n",
        "    [2, 3, -1],\n",
        "])\n",
        "\n",
        "test_input_unformatted = \"11111\"  # 11111 = all variables True\n",
        "test_input = format_vars(test_input_unformatted)  # one-hot-encoded bools\n",
        "\n",
        "\n",
        "program = verify_3sat(clauses, n_variables)\n",
        "max_seq_len = n_variables*2\n",
        "vocab = set([1,0])\n",
        "tracr_model = compiling.compile_rasp_to_model(program, vocab, max_seq_len, False, compiler_bos, compiler_pad)\n",
        "run_tracr_model = create_tracr_runner_fn(tracr_model)\n",
        "\n",
        "print(\"test input:\", test_input)\n",
        "decoded_output = run_tracr_model(test_input)\n",
        "print(\"decoded_output:\", decoded_output)"
      ],
      "metadata": {
        "id": "rG1ljuhkdPVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test the correctness of the Tracr program by comparing its outputs to a 3-SAT checker in Python."
      ],
      "metadata": {
        "id": "WCMZ4NPxMBx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def sat_tester(clauses, variables):\n",
        "  for c in clauses:\n",
        "    found_satisfying_variable = False\n",
        "    for i in range(3):\n",
        "      c_needs_True = c[i] > 0\n",
        "      c_ith_variable_index = int(abs(c[i])-1)\n",
        "      v_is_True = int(variables[c_ith_variable_index]) > 0.5\n",
        "      # print(c,i,c_needs_True == v_is_True)\n",
        "      if c_needs_True == v_is_True:\n",
        "        found_satisfying_variable = True\n",
        "        break\n",
        "    if found_satisfying_variable == False:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def generate_binary_strings(n):\n",
        "    return [''.join(bits) for bits in product('01', repeat=n)]\n",
        "\n",
        "def test_tracr_3sat_correctness():\n",
        "  binary_strings = generate_binary_strings(n_variables)\n",
        "  n_sat_assignments = 0\n",
        "  for s in binary_strings:\n",
        "    formatted_s = format_vars(s)\n",
        "    tracr_result = run_tracr_model(formatted_s)[-1]\n",
        "    sat_tester_result = sat_tester(clauses, s)\n",
        "    if sat_tester_result!=tracr_result:\n",
        "      print(f\"mismatch: {s}, tester:{sat_tester_result}, tracr:{tracr_result}\")\n",
        "    if sat_tester_result:\n",
        "      # print(f\"Satisfying: {s}\")\n",
        "      n_sat_assignments+=1\n",
        "    # assert sat_tester_result==tracr_result\n",
        "  print(f\"All {len(binary_strings)} binary strings passed the test\")\n",
        "  print(f\"Nr of satisfying assignments: {n_sat_assignments}/{len(binary_strings)}\")\n",
        "\n",
        "sat_tester_result = sat_tester(clauses, test_input_unformatted)\n",
        "print(\"sat_tester on test_input:\", sat_tester_result)\n",
        "test_tracr_3sat_correctness()"
      ],
      "metadata": {
        "id": "5VOitpyW7dbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It works!"
      ],
      "metadata": {
        "id": "4oUSbOQ2Z7cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a different RASP program, you can overwrite the relevant variables (program, max_seq_len,vocab, tracr_model, run_tracr_model) and the rest of the notebook should work."
      ],
      "metadata": {
        "id": "A0fHLa-sXnUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the TransformerLens model"
      ],
      "metadata": {
        "id": "hlmxIPcH1NUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we extract Tracr/Craft parameters. This solves a bug with manually setting the TransformerLens unembed matrix, where it stopped working if the input space was different from the output space."
      ],
      "metadata": {
        "id": "6LBOzS6iMwdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get Tracr parameters\n",
        "from tracr.compiler import rasp_to_graph, nodes, craft_graph_to_model, basis_inference, expr_to_craft_graph\n",
        "from tracr.craft import bases, vectorspace_fns\n",
        "\n",
        "def get_unembed_params():\n",
        "  mlp_exactness = 100  # Controls the approximation of the MLP layers.\n",
        "  extracted = rasp_to_graph.extract_rasp_graph(program)\n",
        "  graph, sources, sink = extracted.graph, extracted.sources, extracted.sink\n",
        "  basis_inference.infer_bases(graph, sink, vocab, max_seq_len,)\n",
        "  expr_to_craft_graph.add_craft_components_to_rasp_graph(graph, bos_dir=bases.BasisDirection(rasp.tokens.label, compiler_bos), mlp_exactness=mlp_exactness,)\n",
        "  craft_model = craft_graph_to_model.craft_graph_to_model(graph, sources)\n",
        "  tokens_value_set = (graph.nodes[rasp.tokens.label][nodes.VALUE_SET].union({compiler_bos, compiler_pad}))\n",
        "  tokens_space = bases.VectorSpaceWithBasis.from_values(rasp.tokens.label, tokens_value_set)\n",
        "  indices_space = bases.VectorSpaceWithBasis.from_values(rasp.indices.label, range(max_seq_len))\n",
        "  categorical_output = rasp.is_categorical(sink[nodes.EXPR])\n",
        "  output_space = bases.VectorSpaceWithBasis(sink[nodes.OUTPUT_BASIS])\n",
        "  residual_space = bases.join_vector_spaces(craft_model.residual_space, tokens_space, indices_space, output_space)\n",
        "  res_to_out = vectorspace_fns.project(residual_space, output_space)\n",
        "  return res_to_out.matrix, len(output_space.basis)\n",
        "\n",
        "tracr_unembed_matrix, tracr_d_vocab_out = get_unembed_params()"
      ],
      "metadata": {
        "id": "Ey_XIQAeHkUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAzhqiTOm3Zw"
      },
      "source": [
        "Extract the model config from the Tracr model, and create a blank HookedTransformer object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUh1RWkJm3Zw"
      },
      "outputs": [],
      "source": [
        "#@title Define TransformerLens hyperparameters\n",
        "\n",
        "n_heads = tracr_model.model_config.num_heads\n",
        "n_layers = tracr_model.model_config.num_layers\n",
        "d_head = tracr_model.model_config.key_size\n",
        "d_mlp = tracr_model.model_config.mlp_hidden_size\n",
        "act_fn = \"relu\"\n",
        "normalization_type = \"LN\"  if tracr_model.model_config.layer_norm else None\n",
        "attention_type = \"causal\"  if tracr_model.model_config.causal else \"bidirectional\"\n",
        "\n",
        "n_ctx = tracr_model.params[\"pos_embed\"]['embeddings'].shape[0]\n",
        "# Equivalent to length of vocab, with BOS and PAD at the end\n",
        "d_vocab = tracr_model.params[\"token_embed\"]['embeddings'].shape[0]\n",
        "# Residual stream width, I don't know of an easy way to infer it from the above config.\n",
        "d_model = tracr_model.params[\"token_embed\"]['embeddings'].shape[1]\n",
        "\n",
        "# Equivalent to length of vocab, WITHOUT BOS and PAD at the end because we never care about these outputs\n",
        "# In practice, we always feed the logits into an argmax\n",
        "# d_vocab_out = tracr_model.params[\"token_embed\"]['embeddings'].shape[0] - 2  # incorrect! fixed below\n",
        "d_vocab_out = tracr_d_vocab_out\n",
        "\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers=n_layers,\n",
        "    d_model=d_model,\n",
        "    d_head=d_head,\n",
        "    n_ctx=n_ctx,\n",
        "    d_vocab=d_vocab,\n",
        "    d_vocab_out=d_vocab_out,\n",
        "    d_mlp=d_mlp,\n",
        "    n_heads=n_heads,\n",
        "    act_fn=act_fn,\n",
        "    attention_dir=attention_type,\n",
        "    normalization_type=normalization_type,\n",
        ")\n",
        "tl_model = HookedTransformer(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFIWOWBOm3Zw"
      },
      "source": [
        "Extract the state dict, and do some reshaping so that everything has a n_heads dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk6KSLM0m3Zw"
      },
      "outputs": [],
      "source": [
        "#@title Construct TransformerLens model\n",
        "# %%\n",
        "sd = {}\n",
        "sd[\"pos_embed.W_pos\"] = tracr_model.params[\"pos_embed\"]['embeddings']\n",
        "sd[\"embed.W_E\"] = tracr_model.params[\"token_embed\"]['embeddings']\n",
        "# Equivalent to max_seq_len plus one, for the BOS\n",
        "\n",
        "# The unembed is just a projection onto the first few elements of the residual stream, these store output tokens\n",
        "# This is a NumPy array, the rest are Jax Arrays, but w/e it's fine.\n",
        "# sd[\"unembed.W_U\"] = np.eye(d_model, d_vocab_out)  # incorrect! fixed below\n",
        "sd[\"unembed.W_U\"] = tracr_unembed_matrix\n",
        "\n",
        "\n",
        "for l in range(n_layers):\n",
        "    sd[f\"blocks.{l}.attn.W_K\"] = einops.rearrange(\n",
        "        tracr_model.params[f\"transformer/layer_{l}/attn/key\"][\"w\"],\n",
        "        \"d_model (n_heads d_head) -> n_heads d_model d_head\",\n",
        "        d_head = d_head,\n",
        "        n_heads = n_heads\n",
        "    )\n",
        "    sd[f\"blocks.{l}.attn.b_K\"] = einops.rearrange(\n",
        "        tracr_model.params[f\"transformer/layer_{l}/attn/key\"][\"b\"],\n",
        "        \"(n_heads d_head) -> n_heads d_head\",\n",
        "        d_head = d_head,\n",
        "        n_heads = n_heads\n",
        "    )\n",
        "    sd[f\"blocks.{l}.attn.W_Q\"] = einops.rearrange(\n",
        "        tracr_model.params[f\"transformer/layer_{l}/attn/query\"][\"w\"],\n",
        "        \"d_model (n_heads d_head) -> n_heads d_model d_head\",\n",
        "        d_head = d_head,\n",
        "        n_heads = n_heads\n",
        "    )\n",
        "    sd[f\"blocks.{l}.attn.b_Q\"] = einops.rearrange(\n",
        "        tracr_model.params[f\"transformer/layer_{l}/attn/query\"][\"b\"],\n",
        "        \"(n_heads d_head) -> n_heads d_head\",\n",
        "        d_head = d_head,\n",
        "        n_heads = n_heads\n",
        "    )\n",
        "    sd[f\"blocks.{l}.attn.W_V\"] = einops.rearrange(\n",
        "        tracr_model.params[f\"transformer/layer_{l}/attn/value\"][\"w\"],\n",
        "        \"d_model (n_heads d_head) -> n_heads d_model d_head\",\n",
        "        d_head = d_head,\n",
        "        n_heads = n_heads\n",
        "    )\n",
        "    sd[f\"blocks.{l}.attn.b_V\"] = einops.rearrange(\n",
        "        tracr_model.params[f\"transformer/layer_{l}/attn/value\"][\"b\"],\n",
        "        \"(n_heads d_head) -> n_heads d_head\",\n",
        "        d_head = d_head,\n",
        "        n_heads = n_heads\n",
        "    )\n",
        "    sd[f\"blocks.{l}.attn.W_O\"] = einops.rearrange(\n",
        "        tracr_model.params[f\"transformer/layer_{l}/attn/linear\"][\"w\"],\n",
        "        \"(n_heads d_head) d_model -> n_heads d_head d_model\",\n",
        "        d_head = d_head,\n",
        "        n_heads = n_heads\n",
        "    )\n",
        "    sd[f\"blocks.{l}.attn.b_O\"] = tracr_model.params[f\"transformer/layer_{l}/attn/linear\"][\"b\"]\n",
        "\n",
        "    sd[f\"blocks.{l}.mlp.W_in\"] = tracr_model.params[f\"transformer/layer_{l}/mlp/linear_1\"][\"w\"]\n",
        "    sd[f\"blocks.{l}.mlp.b_in\"] = tracr_model.params[f\"transformer/layer_{l}/mlp/linear_1\"][\"b\"]\n",
        "    sd[f\"blocks.{l}.mlp.W_out\"] = tracr_model.params[f\"transformer/layer_{l}/mlp/linear_2\"][\"w\"]\n",
        "    sd[f\"blocks.{l}.mlp.b_out\"] = tracr_model.params[f\"transformer/layer_{l}/mlp/linear_2\"][\"b\"]\n",
        "print(sd.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Manq1uVm3Zx"
      },
      "outputs": [],
      "source": [
        "#@title Convert weights to tensors and load into the tl_model\n",
        "# %%capture\n",
        "for k, v in sd.items():\n",
        "    sd[k] = torch.tensor(np.array(v))\n",
        "\n",
        "tl_model.load_state_dict(sd, strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GwAVsGqm3Zx"
      },
      "outputs": [],
      "source": [
        "#@title Create helper functions to do the tokenization and de-tokenization\n",
        "# %%\n",
        "INPUT_ENCODER = tracr_model.input_encoder\n",
        "OUTPUT_ENCODER = tracr_model.output_encoder\n",
        "\n",
        "def create_model_input(input, input_encoder=INPUT_ENCODER):\n",
        "    encoding = input_encoder.encode(input)\n",
        "    return torch.tensor(encoding).unsqueeze(dim=0)\n",
        "\n",
        "def decode_model_output(logits, output_encoder=OUTPUT_ENCODER, bos_token=INPUT_ENCODER.bos_token):\n",
        "    max_output_indices = logits.squeeze(dim=0).argmax(dim=-1)\n",
        "    decoded_output = output_encoder.decode(max_output_indices.tolist())\n",
        "    decoded_output_with_bos = [bos_token] + decoded_output[1:]\n",
        "    return decoded_output_with_bos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMJd8S2Jm3Zx"
      },
      "outputs": [],
      "source": [
        "#@title We can now run the model!\n",
        "def get_tl_input_tokens_tensor(input: Iterable)-> torch.Tensor:\n",
        "  input_list = create_input_list(input)\n",
        "  return create_model_input(input_list)\n",
        "\n",
        "def run_tl_model(input: Iterable)-> list:\n",
        "  tl_input_tokens_tensor = get_tl_input_tokens_tensor(input)\n",
        "  tl_logits = tl_model(tl_input_tokens_tensor)\n",
        "  tl_decoded_output = decode_model_output(tl_logits)\n",
        "  return tl_decoded_output\n",
        "\n",
        "tracr_decoded_output = run_tracr_model(test_input)\n",
        "tl_decoded_output = run_tl_model(test_input)\n",
        "print(\"Original Tracr Decoding: \", tracr_decoded_output)\n",
        "print(\"TransformerLens Decoding:\", tl_decoded_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsSF-BzSm3Zx"
      },
      "outputs": [],
      "source": [
        "#@title Check whether the activations and outputs match\n",
        "tracr_out = tracr_model.apply(create_input_list(test_input))\n",
        "tl_input_tokens_tensor = get_tl_input_tokens_tensor(test_input)\n",
        "logits, cache = tl_model.run_with_cache(tl_input_tokens_tensor)\n",
        "# (cached all intermediate activations in the model)\n",
        "\n",
        "for layer in range(tl_model.cfg.n_layers):\n",
        "    attention_equal = np.isclose(cache[\"attn_out\", layer].detach().cpu().numpy(), np.array(tracr_out.layer_outputs[2*layer])).all()\n",
        "    mlp_equal = np.isclose(cache[\"mlp_out\", layer].detach().cpu().numpy(), np.array(tracr_out.layer_outputs[2*layer+1])).all()\n",
        "    assert(attention_equal and mlp_equal)\n",
        "    print(f\"Layer {layer} Attn Out Equality Check:\", attention_equal)\n",
        "    print(f\"Layer {layer} MLP Out Equality Check: \", mlp_equal)\n",
        "\n",
        "outputs_match = tracr_decoded_output == tl_decoded_output\n",
        "print(\"Outputs match:\", outputs_match)\n",
        "assert(outputs_match)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_activations = cache[\"mlp_out\", -1].detach().cpu().numpy()[0]\n",
        "# plt.imshow(test_activations, interpolation='none')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "_cOZ6Ybz0N_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TransformerLens Model\n",
        "tl_model"
      ],
      "metadata": {
        "id": "x9xranoHOpCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run tests with TransformerLens and Tracr models"
      ],
      "metadata": {
        "id": "h7aC0dS41bJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Forward pass\n",
        "# test_input = \"password\"\n",
        "# test_input = \"1101\"\n",
        "tracr_output = run_tracr_model(test_input)\n",
        "tl_output = run_tl_model(test_input)\n",
        "print(tracr_output)\n",
        "print(tl_output)"
      ],
      "metadata": {
        "id": "tf73ghw4E3kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCB5fMwWm3Zx"
      },
      "source": [
        "The positions on the vertical axis are all tokens from the output vocab (which is automatically inferred and might be different from input vocab). The horizontal axis is the dimensions of the final residual stream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSfbg9Vqm3Zx"
      },
      "outputs": [],
      "source": [
        "#@title Plot the final residual stream\n",
        "\n",
        "import plotly.express as px\n",
        "final_residual_stream = cache[\"resid_post\", -1].detach().cpu().numpy()[0]\n",
        "# px.imshow(final_residual_stream,\n",
        "# color_continuous_scale=\"Blues\", labels={\"x\":\"Residual Stream\", \"y\":\"Position\"}, y=[str(i) for i in create_input_list(test_input)]).show(\"colab\" if IN_COLAB else \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99FBiGH7bsfn"
      },
      "source": [
        "# Setting up visualisations\n",
        "\n",
        "This part is adapted from https://github.com/google-deepmind/tracr/blob/main/tracr/examples/Visualize_Tracr_Models.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtOAc_yWawFR"
      },
      "outputs": [],
      "source": [
        "#@title Plotting functions\n",
        "\n",
        "plot_height = final_residual_stream.shape[1]//3\n",
        "plot_width = final_residual_stream.shape[0]\n",
        "\n",
        "def tidy_label(label, value_width=5):\n",
        "  if ':' in label:\n",
        "    label, value = label.split(':')\n",
        "  else:\n",
        "    value = ''\n",
        "  return label + f\":{value:>{value_width}}\"\n",
        "\n",
        "\n",
        "def add_residual_ticks(model, value_width=5, x=False, y=True):\n",
        "  if y:\n",
        "    plt.yticks(\n",
        "            np.arange(len(model.residual_labels))+0.5,\n",
        "            [tidy_label(l, value_width=value_width)\n",
        "              for l in model.residual_labels],\n",
        "            family='monospace',\n",
        "            fontsize=20,\n",
        "    )\n",
        "  if x:\n",
        "    plt.xticks(\n",
        "            np.arange(len(model.residual_labels))+0.5,\n",
        "            [tidy_label(l, value_width=value_width)\n",
        "              for l in model.residual_labels],\n",
        "            family='monospace',\n",
        "            rotation=90,\n",
        "            fontsize=20,\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_computation_trace(model,\n",
        "                           input_labels,\n",
        "                           residuals_or_outputs,\n",
        "                           add_input_layer=False,\n",
        "                           figsize=(plot_width, plot_height),\n",
        "                           show_last_n_layers=None,\n",
        "                           residual_original_len=None):\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=len(residuals_or_outputs), figsize=figsize, sharey=True)\n",
        "  value_width = max(map(len, map(str, input_labels))) + 1\n",
        "\n",
        "  for i, (layer, ax) in enumerate(zip(residuals_or_outputs, axes)):\n",
        "    ax.grid(True, lw=0.5)\n",
        "    plt.sca(ax)\n",
        "    plt.pcolormesh(layer[0].T, vmin=0, vmax=1)\n",
        "    if i == 0:\n",
        "      add_residual_ticks(model, value_width=value_width)\n",
        "    if show_last_n_layers is not None:\n",
        "      plt.xticks(\n",
        "          np.arange(len(input_labels))+0.5,\n",
        "          input_labels,\n",
        "          rotation=90,\n",
        "          fontsize=20,\n",
        "      )\n",
        "    if add_input_layer and i == 0:\n",
        "      if show_last_n_layers is not None:\n",
        "        title = 'Input'\n",
        "      else:\n",
        "        title = 'in'\n",
        "    else:\n",
        "      layer_no = i - 1 if add_input_layer else i\n",
        "      if show_last_n_layers is not None:\n",
        "        layer_type = 'Attn' if layer_no % 2 == 0 else 'MLP'\n",
        "        title = f'{layer_type}{(residual_original_len-show_last_n_layers)//2 + layer_no // 2 + 1}'\n",
        "      else:\n",
        "        layer_type = 'A' if layer_no % 2 == 0 else 'M'\n",
        "        title = f'{layer_type}{layer_no // 2 + 1}'\n",
        "    plt.title(title, fontsize=20)\n",
        "\n",
        "\n",
        "def plot_residuals_and_input(model, inputs, figsize=(plot_width, plot_height), show_last_n_layers=None):\n",
        "  \"\"\"Applies model to inputs, and plots the residual stream at each layer.\"\"\"\n",
        "  model_out = model.apply(inputs)\n",
        "  residuals = np.concatenate([model_out.input_embeddings[None, ...],\n",
        "                              model_out.residuals], axis=0)\n",
        "  if show_last_n_layers is not None:\n",
        "    residual_original_len = len(residuals)\n",
        "    residuals = residuals[-show_last_n_layers:]\n",
        "    add_input_layer = False\n",
        "  else:\n",
        "    residual_original_len = None\n",
        "    add_input_layer = True\n",
        "  plot_computation_trace(\n",
        "      model=model,\n",
        "      input_labels=inputs,\n",
        "      residuals_or_outputs=residuals,\n",
        "      add_input_layer=add_input_layer,\n",
        "      figsize=figsize,\n",
        "      show_last_n_layers=show_last_n_layers,\n",
        "      residual_original_len=residual_original_len)\n",
        "\n",
        "\n",
        "def plot_layer_outputs(model, inputs, figsize=(plot_width, plot_height), show_last_n_layers=None):\n",
        "  \"\"\"Applies model to inputs, and plots the outputs of each layer.\"\"\"\n",
        "  model_out = model.apply(inputs)\n",
        "  plot_computation_trace(\n",
        "      model=model,\n",
        "      input_labels=inputs,\n",
        "      residuals_or_outputs=model_out.layer_outputs,\n",
        "      add_input_layer=False,\n",
        "      figsize=figsize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtwiE-JiXF3F",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Forward pass\n",
        "# test_input_list = [\"bos\", 3, 4, 1]\n",
        "test_input_2 = test_input[:]\n",
        "# test_input_2 = \"1000\"\n",
        "test_input_list = create_input_list(test_input_2)\n",
        "print(test_input_list, \"-inputs\")\n",
        "tracr_model.apply(test_input_list).decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Tracr Models"
      ],
      "metadata": {
        "id": "6sfLgS68BgFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot final residual stream\n",
        "plot_residuals_and_input(model=tracr_model, inputs=test_input_list, show_last_n_layers=2)"
      ],
      "metadata": {
        "id": "2q9-HevL_HOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkEkVcEHa2gf"
      },
      "outputs": [],
      "source": [
        "#@title Plot residual stream\n",
        "# plots the entire stream - can be slow\n",
        "plot_residuals_and_input(model=tracr_model, inputs=test_input_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c4LakWHa4ey"
      },
      "outputs": [],
      "source": [
        "#@title Plot layer outputs\n",
        "# plot_layer_outputs(model=tracr_model, inputs = test_input_list)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hlmxIPcH1NUY",
        "h7aC0dS41bJi"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "python38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "d6970b9bc1f22c1555ce2e3aef3e9bc8c56c5727cd75cae357902c75ead4068e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}